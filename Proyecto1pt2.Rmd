---
title: "Proyecto1pt2"
output: html_document
date: "2025-02-10"
---

```{r setup, include=FALSE, message=FALSE,warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)

library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
library(knitr)
library(dplyr)
library(psych)
library(FactoMineR)
library(fpc)
library(factoextra)
library(corrplot)
library(PCAmixdata)
library(paran)

movies <- read.csv("./movies.csv")
```

# Clustering
## Procesamiento del dataset

Para este apartado se decidio calcular los grupos en base a todas las variables numericas, quitando todas las variables no son de este tipo. Para posteriormente poder clasificar o ver la forma en la que se relacionan las variables por medio de estas que fueron descartadas en un inicio.

Las variables con las que se trabajara son:
- popularity: 
- Budget:
- Revenue

```{r data without string, echo=FALSE, message=FALSE}


movies$actorsPopularityMean <- sapply(movies$actorsPopularity, function(x) {
  values <- as.numeric(unlist(strsplit(x, "\\|")))
  mean(values, na.rm = TRUE)
})

movies$castMenAmount <- as.numeric(movies$castMenAmount)

movies$castWomenAmount <- as.numeric(movies$castWomenAmount)

#datos <- movies[,c("popularity", "budget", "revenue", "runtime", "genresAmount", "productionCoAmount", "productionCountriesAmount", "voteCount", "voteAvg", "castMenAmount", "castWomenAmount")]

datos <- movies[,c("popularity", "budget", "revenue", "runtime", "genresAmount", "voteCount", "voteAvg", "castMenAmount", "castWomenAmount")]


datos <- scale(datos)
datos <- na.omit(datos)


set.seed(123)
vhopkins <- hopkins(datos)
```

Primero necesitaremos verificar si vale la pena agrupar los datos. Usando el estadistico de hopkings nos dio un resultado de `r vhopkins`, lo que indica que los resultados no son aleatorios y hay una alta posibilidad de que sea factible el agrupamiento.

Posteriormente se realizo un mapa de calor parar verificar si realmente existen patrones.
```{r VAT, echo=FALSE}
datos_dist<- dist(datos[1:1000,])
fviz_dist(datos_dist, show_labels = F)
```
Como se puede observar en la gráfica observamos que hay cierta relaciones de los datos y a simple vista podemos observar que existen 3 formas de agrupar los datos


```{r }
wss=0
for (i in 1:10) 
  wss[i] <- sum(kmeans(datos[1:1000,], centers=i)$withinss)

plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
```

Se puede observar que en la grafica por los valores 2 - 4 se empieza a doblar el grafica por lo que se usara 3 el cual es un valor medio de estos dos

```{r k medias}
fviz_nbclust(datos[1:1000,], kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
```

```{r kmeans}
km<-kmeans(datos,2,iter.max =100)
#datos$
#datos$grupoKm<-km$cluster
```
```{r k plot}
plotcluster(datos,km$cluster)
```


```{r plot2}
fviz_cluster(km, data = datos,geom = "point", ellipse.type = "norm")
```



```{r a}
silkm<-silhouette(km$cluster,dist(datos))
plot(silkm, cex.names=.4, col=1:3)
```

--- 

# Analisis por Componentes principales

Para ello primero se requirio de la limpieza del dataset para contar solamente contar con un dataframe de variables cuantitativas. Este sería un ejemplo del Dataset con el que se trabajará el PCA.

Para ello se tomaron solo variables que eran inherentemente cuantitativas, ya que a pesar que las variables cualitativas aunque no se conviertan a un equivalente númerico, no tienen noción de orden y magnitud, y por en no aportando mucho valor al cálculo de componentes.

```{r}
PCA_raw_data_na <- na.omit(movies[,c("popularity", "budget", "revenue", "runtime", "genresAmount", "voteCount", "voteAvg", "castMenAmount", "castWomenAmount")])
PCA_raw_data_na$castMenAmount <- as.numeric(PCA_raw_data_na$castMenAmount)
PCA_raw_data_na$castWomenAmount <- as.numeric(PCA_raw_data_na$castWomenAmount)
PCA_raw_data <- PCA_raw_data_na[complete.cases(PCA_raw_data_na), ]
kable(head(PCA_raw_data))
```

## PCA es aplicable
Acto seguido, es importantes verificar si el PCA será una buena técnica para este conjunto de datos. Empezando por revisar la colinealidad de las variables: 

```{r}
rcor<-cor(PCA_raw_data, use = "pairwise.complete.obs")
corrplot(rcor)
```
Vemos un valor es `r det(rcor)`, que es cercano a 0, y también podemos ver que hay una relación del estrecha (cercana al 60%) entre las ganancias y el presupuesto, así como número de votos y ganancias. Para reafirmar si el PCA será un método correcto, se corrieron tambien los test KMO y Test de esfericidad de Bartlet que arrojan los siguientes indicios:

**KMO**
```{r}
KMO(as.matrix(PCA_raw_data))
```
**Esfericidad de Bartlet**
```{r}
cortest.bartlett(PCA_raw_data)
```
Ambas estadísticas ofrecen resultados favorecedores al indica que si hay relación entre las diferentes variables y datos con con un MSA superior a 0.5 (MSA=0.71) y bartlet indicando que si hay interdependencia entre las variables (p=0). Es decir, podemos procer a aplicar PCA.


## Aplicación de PCA

Estas serian las componentes de principales del subset de datos.
```{r}
sum(is.na(PCA_raw_data))
compPrinc<-prcomp(PCA_raw_data, scale = T)
compPrinc
```
Por la regla de Kaiser (las componentes con desviacion estandar mayor a 1 son las que aportan mayor peso) se desvela que las primeras 77% componentes son las más importantes al explicar el 86% de la variación de los datos:

```{r}
summary(compPrinc)
```
La gŕafica recomienda el uso de 2 componentes, pero para encontrar, eso explicaria solo el 44% de la variabilidad de los datos, así que nos quedaremos en el análisis con componentes: 
```{r screeplot}
fviz_eig(compPrinc, addlabels = TRUE, ylim = c(0, 80))
fviz_eig(compPrinc, addlabels = TRUE, choice = c("eigenvalue"), ylim = c(0, 3))
```
## Interpretación de PSA

El siguiente gráfico muestra vectores proyectos sobre las 2 primeras componentes. Y se puede observar las relaciones descubiertas en pasos anteriorres sobre la covarianza: Las componetnes de presupuesto, y número de votos y ganancias estan muy relacionadas positivamente entre, y bien representadas en la primera dimensión.

También se puede decir algo parecido del tiempo de expiración y el promedio de votos, solo que estos 2 ultimos se encuentran menos representados en la 1 y 2 dimensión.
```{r grafico coseno}
fviz_pca_var(compPrinc, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
)

```
