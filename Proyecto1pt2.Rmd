---
title: "Proyecto1"
author: "Gerardo Pineda 22880, Daniel Rayo 22933"
date: "2025-02-10"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE, message=FALSE,warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
Sys.setlocale("LC_ALL", "es_ES.UTF-8")

library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
library(knitr)
library(dplyr)
library(psych)
library(FactoMineR)
library(fpc)
library(dplyr)
library(factoextra)
library(corrplot)
library(PCAmixdata)
library(paran)

movies <- read.csv("./movies.csv")
```

# Clustering
## Procesamiento del dataset

Para este apartado se decidio calcular los grupos en base a todas las variables numericas, quitando todas las variables no son de este tipo. Para posteriormente poder clasificar o ver la forma en la que se relacionan las variables por medio de estas que fueron descartadas en un inicio.

Las variables con las que se trabajara los clusters son:  
* popularity: Importante para agrupar películas por impacto mediático.
* Budget: Ayuda a diferenciar producciones de alto y bajo costo.
* Revenue: Relacionado con el éxito comercial.
* runtime: Útil para agrupar películas cortas vs. largas.
* genresAmount: Más géneros pueden significar una audiencia más diversa.
* voteCount: Relacionado con la cantidad de personas que la vieron.
* voteAvg: Permite separar películas mejor o peor valoradas.
* castMenAmount y castWomenAmount: Para analizar el tamaño del reparto y su diversidad.

```{r data without string, echo=FALSE, message=FALSE}


movies$actorsPopularityMean <- sapply(movies$actorsPopularity, function(x) {
  values <- as.numeric(unlist(strsplit(x, "\\|")))
  mean(values, na.rm = TRUE)
})

movies$castMenAmount <- as.numeric(movies$castMenAmount)

movies$castWomenAmount <- as.numeric(movies$castWomenAmount)


datos <- movies[,c("popularity", "budget", "revenue", "runtime", "genresAmount", "voteCount", "voteAvg", "castMenAmount", "castWomenAmount")]


datos <- scale(datos)
datos <- na.omit(datos)


set.seed(123)
vhopkins <- hopkins(datos)
```

Primero necesitaremos verificar si vale la pena agrupar los datos. Usando el estadistico de hopkings nos dio un resultado de `r vhopkins`, lo que indica que los resultados no son aleatorios y hay una alta posibilidad de que sea factible el agrupamiento.

Posteriormente se realizo un mapa de calor parar verificar si realmente existen patrones.  

```{r VAT, echo=FALSE, warning=FALSE, message=FALSE}
datos_dist<- dist(datos[1:1000,])
fviz_dist(datos_dist, show_labels = F)
```
  
El mapa de calor revela que existen patrones claros en los datos, con al menos tres grupos de observaciones con similitudes internas. Esto refuerza la idea de que el dataset puede dividirse en tres clusters. La presencia de áreas con colores más intensos sugiere que algunas observaciones son más similares entre sí en ciertas características, mientras que otras tienen mayor variabilidad. El dataSet puede ser bueno para representar diferentes tipos de películas en términos de su presupuesto, popularidad y desempeño en taquilla.


```{r codo}
wss=0
for (i in 1:10) 
  wss[i] <- sum(kmeans(datos[1:1000,], centers=i)$withinss)

plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
```

El método del codo sugiere que un número óptimo de clusters es k = 3, ya que en este punto la reducción en la varianza dentro de los clusters deja de ser significativa.

```{r k medias}
fviz_nbclust(datos[1:1000,], kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
```

## K means
Antes de empezar con el k mean comprobamos cual k era el mas adecuado para este clustering  

### k = 3

```{r k-means, echo=FALSE}
km <- kmeans(datos, centers = 3, iter.max = 100)


if (!is.data.frame(datos)) {
  datos <- as.data.frame(datos)
}

datos$grupoKm <- as.factor(km$cluster)

dataWithoutGroups <- subset(datos, select = -grupoKm)
```
```{r plot cluster groups, echo=FALSE}
plotcluster(dataWithoutGroups,km$cluster)
```


```{r plot cluster groups 2, echo=FALSE}
fviz_cluster(km, data = dataWithoutGroups,geom = "point", ellipse.type = "norm")
```

```{r silueta k mean, echo=FALSE}
silkm<-silhouette(km$cluster,dist(dataWithoutGroups))
plot(silkm, cex.names=.4, col=1:3)
```

### K = 2
```{r k-means2, echo=FALSE}
km <- kmeans(datos, centers = 2, iter.max = 100)

# Convertir a data.frame si es necesario
if (!is.data.frame(datos)) {
  datos <- as.data.frame(datos)
}

# Agregar la columna con los clusters
datos$grupoKm <- as.factor(km$cluster)

dataWithoutGroups <- subset(datos, select = -grupoKm)
```
```{r plot cluster groups2, echo=FALSE}
plotcluster(dataWithoutGroups,km$cluster)
```

```{r silueta k mean2, echo=FALSE}
silkm<-silhouette(km$cluster,dist(dataWithoutGroups))
plot(silkm, cex.names=.4, col=1:3)
```
  
Aunque el método del codo sugirió que k=3 podría ser un punto adecuado, al calcular el índice de silueta encontramos que para k=2 el valor es 0.55, significativamente mayor que el 0.18 obtenido con k=3. Esto indica que los clusters son más compactos y están mejor separados cuando se utilizan dos grupos en lugar de tres. Por esta razón, elegimos k=2, ya que proporciona una segmentación más clara y efectiva de los datos.  


## Cluster Jerarquico

```{r cluset jerarquico, echo=FALSE}

matriz_dist<- dist(dataWithoutGroups)
datos_dist<- dist(dataWithoutGroups)
hc<-hclust(datos_dist, method = "ward.D2")
plot(hc, cex=0.5, axes=FALSE)
rect.hclust(hc,k=3)
```

Aunque con k=2 la silueta fue mejor en K-means, al analizar el dendrograma del clustering jerárquico, se observó que una división natural ocurre en tres grupos. Esto sugiere que, en lugar de forzar una separación en dos clusters, el modelo jerárquico es capaz de detectar subestructuras en los datos que K-means no capturó.

```{r jerarquico, echo=FALSE}
#groups<-cutree(hc,k=3)

#datos$gruposHC<-groups


#table(groups)
```

```{r silueta jerarquico, echo=FALSE}
silhc<-silhouette(groups,datos_dist)
mean(silhc[,3]) 
```


```{r grafica, echo=FALSE}
plot(silhc, cex.names=.4, col=1:3)

```


## Interpretacion del clustering

*Comparación de resultados:*
```{r comparacion}
tabla_resultados <- data.frame(
  Algoritmo = c("K-means", "Jerárquico"),
  `Número de clusters` = c(2, 3),
  `Índice de silueta` = c(0.55, 0.51),
  Desciprció = c(
    "Mayor cohesión y separación clara",
    "El dendrograma sugiere 3 subgrupos bien diferenciados"
  )
)

# Mostrar la tabla en formato bonito en RMarkdown
kable(tabla_resultados)
```

Resumen de las variables con el metodo de k means
```{r summary de los datos, echo=FALSE, warning=FALSE}

movies <- movies[rownames(movies) %in% rownames(datos), ]

movies$grupoKm <- datos$grupoKm  
movies$grupoHC <- datos$gruposHC 


# Resumen estadístico por grupo
movies <- as.data.frame(movies)
movies_clean <- movies %>% dplyr::select(-grupoHC)

summary_by_group <- movies_clean %>%
  group_by(grupoKm) %>%
  summarise(across(where(is.numeric), list(mean = mean, sd = sd), .names = "{.col}_{.fn}"), na.rm = TRUE)


variables_importantes <- c("popularity_mean", "budget_mean", "revenue_mean", "runtime_mean")

kable(summary_by_group %>% dplyr::select(all_of(variables_importantes)))

```

Resumen de las variables con el metodo de cluster jerarquico
```{r summary de los datos jerarquicos, echo=FALSE, warning=FALSE}
summary_by_groupHC <- movies %>%
  group_by(grupoHC) %>%
  summarise(across(where(is.numeric), list(mean = mean, sd = sd), .names = "{.col}_{.fn}"))


variables_importantes <- c("popularity_mean", "budget_mean", "revenue_mean", "runtime_mean")

kable(summary_by_groupHC %>% dplyr::select(all_of(variables_importantes)))
```


K mean por el meotdo de la silueta le quedaba de mejor forma organizarse en 2 grupos y cluster jerarquico por este mismo metodo le quedaba de mejor forma organizarse en 3 grupos.

Teniendo eso en cuenta analizamos procedemos a analizar.

Por lo que se puede observar en el resumen de ambos grupos, los dos metodos tuvieron distribuciones bastante parecidas menos por el presupuesto y los ingresos, los cuales por claras razones de ser 3 y 2 en el cluster jerarquico estan un poco mejor distribuidas.

```{r prespuesto vs revenues, echo=FALSE, warning=FALSE, error=FALSE}
ggplot(movies, aes(x = budget, y = revenue, color = as.factor(grupoKm))) +
  geom_point(alpha = 0.6) +
  labs(title = "Relación entre Presupuesto y Revenue por Cluster", x = "Presupuesto", y = "Revenue") +
  theme_minimal()
```

```{r actor popularidad, echo=FALSE, warning=FALSE, error=FALSE}

movies$actorsPopularityMean <- sapply(movies$actorsPopularity, function(x) {
  values <- as.numeric(unlist(strsplit(x, "\\|"))) 
  mean(values, na.rm = TRUE)
})

ggplot(movies, aes(x = as.factor(grupoKm), y = actorsPopularityMean, fill = as.factor(grupoKm))) +
  geom_boxplot() +
  labs(title = "Popularidad Promedio del Elenco por Cluster", x = "Cluster", y = "Media de Popularidad Actores") +
  theme_minimal()

```

--- 

# Analisis por Componentes principales

Para ello primero se requirio de la limpieza del dataset para contar solamente contar con un dataframe de variables cuantitativas. Este sería un ejemplo del Dataset con el que se trabajará el PCA.

Para ello se tomaron solo variables que eran inherentemente cuantitativas, ya que a pesar que las variables cualitativas aunque no se conviertan a un equivalente númerico, no tienen noción de orden y magnitud, y por en no aportando mucho valor al cálculo de componentes.

```{r}
PCA_raw_data_na <- na.omit(movies[,c("popularity", "budget", "revenue", "runtime", "genresAmount", "voteCount", "voteAvg", "castMenAmount", "castWomenAmount")])
PCA_raw_data_na$castMenAmount <- as.numeric(PCA_raw_data_na$castMenAmount)
PCA_raw_data_na$castWomenAmount <- as.numeric(PCA_raw_data_na$castWomenAmount)
PCA_raw_data <- PCA_raw_data_na[complete.cases(PCA_raw_data_na), ]
kable(head(PCA_raw_data))
```

## PCA es aplicable
Acto seguido, es importantes verificar si el PCA será una buena técnica para este conjunto de datos. Empezando por revisar la colinealidad de las variables: 

```{r}
rcor<-cor(PCA_raw_data, use = "pairwise.complete.obs")
corrplot(rcor)
```
Vemos un valor es `r det(rcor)`, que es cercano a 0, y también podemos ver que hay una relación del estrecha (cercana al 60%) entre las ganancias y el presupuesto, así como número de votos y ganancias. Para reafirmar si el PCA será un método correcto, se corrieron tambien los test KMO y Test de esfericidad de Bartlet que arrojan los siguientes indicios:

**KMO**
```{r}
KMO(as.matrix(PCA_raw_data))
```
**Esfericidad de Bartlet**
```{r}
cortest.bartlett(PCA_raw_data)
```
Ambas estadísticas ofrecen resultados favorecedores al indica que si hay relación entre las diferentes variables y datos con con un MSA superior a 0.5 (MSA=0.71) y bartlet indicando que si hay interdependencia entre las variables (p=0). Es decir, podemos procer a aplicar PCA.


## Aplicación de PCA

Estas serian las componentes de principales del subset de datos.
```{r}
sum(is.na(PCA_raw_data))
compPrinc<-prcomp(PCA_raw_data, scale = T)
compPrinc
```
Por la regla de Kaiser (las componentes con desviacion estandar mayor a 1 son las que aportan mayor peso) se desvela que las primeras 77% componentes son las más importantes al explicar el 86% de la variación de los datos:

```{r}
summary(compPrinc)
```
La gŕafica recomienda el uso de 2 componentes, pero para encontrar, eso explicaria solo el 44% de la variabilidad de los datos, así que nos quedaremos en el análisis con componentes: 
```{r screeplot}
fviz_eig(compPrinc, addlabels = TRUE, ylim = c(0, 80))
fviz_eig(compPrinc, addlabels = TRUE, choice = c("eigenvalue"), ylim = c(0, 3))
```
## Interpretación de PSA

El siguiente gráfico muestra vectores proyectos sobre las 2 primeras componentes. Y se puede observar las relaciones descubiertas en pasos anteriorres sobre la covarianza: Las componetnes de presupuesto, y número de votos y ganancias estan muy relacionadas positivamente entre, y bien representadas en la primera dimensión.

También se puede decir algo parecido del tiempo de expiración y el promedio de votos, solo que estos 2 ultimos se encuentran menos representados en la 1 y 2 dimensión.
```{r grafico coseno}
fviz_pca_var(compPrinc, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
)

```
