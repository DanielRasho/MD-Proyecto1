---
title: "Proyecto1pt2"
output:
  pdf_document: default
  html_document: default
date: "2025-02-10"
---

```{r setup, include=FALSE, message=FALSE,warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)

library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
library(knitr)
library(dplyr)
library(psych)
library(FactoMineR)
library(fpc)
library(dplyr)
library(factoextra)
library(corrplot)
library(PCAmixdata)
library(paran)

movies <- read.csv("./movies.csv")
```

# Clustering
## Procesamiento del dataset

Para este apartado se decidio calcular los grupos en base a todas las variables numericas, quitando todas las variables no son de este tipo. Para posteriormente poder clasificar o ver la forma en la que se relacionan las variables por medio de estas que fueron descartadas en un inicio.

Las variables con las que se trabajara los clusters son:  
* popularity: Importante para agrupar películas por impacto mediático.
* Budget: Ayuda a diferenciar producciones de alto y bajo costo.
* Revenue: Relacionado con el éxito comercial.
* runtime: Útil para agrupar películas cortas vs. largas.
* genresAmount: Más géneros pueden significar una audiencia más diversa.
* voteCount: Relacionado con la cantidad de personas que la vieron.
* voteAvg: Permite separar películas mejor o peor valoradas.
* castMenAmount y castWomenAmount: Para analizar el tamaño del reparto y su diversidad.

```{r data without string, echo=FALSE, message=FALSE}


movies$actorsPopularityMean <- sapply(movies$actorsPopularity, function(x) {
  values <- as.numeric(unlist(strsplit(x, "\\|")))
  mean(values, na.rm = TRUE)
})

movies$castMenAmount <- as.numeric(movies$castMenAmount)

movies$castWomenAmount <- as.numeric(movies$castWomenAmount)

#datos <- movies[,c("popularity", "budget", "revenue", "runtime", "genresAmount", "productionCoAmount", "productionCountriesAmount", "voteCount", "voteAvg", "castMenAmount", "castWomenAmount")]

datos <- movies[,c("popularity", "budget", "revenue", "runtime", "genresAmount", "voteCount", "voteAvg", "castMenAmount", "castWomenAmount")]


datos <- scale(datos)
datos <- na.omit(datos)


set.seed(123)
vhopkins <- hopkins(datos)
```

Primero necesitaremos verificar si vale la pena agrupar los datos. Usando el estadistico de hopkings nos dio un resultado de `r vhopkins`, lo que indica que los resultados no son aleatorios y hay una alta posibilidad de que sea factible el agrupamiento.

Posteriormente se realizo un mapa de calor parar verificar si realmente existen patrones.
```{r VAT, echo=FALSE}
datos_dist<- dist(datos[1:1000,])
fviz_dist(datos_dist, show_labels = F)
```
Como se puede observar en la gráfica observamos que hay cierta relaciones de los datos y a simple vista podemos observar que existen 3 formas de agrupar los datos


```{r }
wss=0
for (i in 1:10) 
  wss[i] <- sum(kmeans(datos[1:1000,], centers=i)$withinss)

plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
```

Se puede observar que en la grafica por los valores 2 - 4 se empieza a doblar el grafica por lo que se usara 3 el cual es un valor medio de estos dos

```{r k medias}
fviz_nbclust(datos[1:1000,], kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
```

```{r k-means, echo=FALSE}
km <- kmeans(datos, centers = 2, iter.max = 100)

# Convertir a data.frame si es necesario
if (!is.data.frame(datos)) {
  datos <- as.data.frame(datos)
}

# Agregar la columna con los clusters
datos$grupoKm <- as.factor(km$cluster)

dataWithoutGroups <- subset(datos, select = -grupoKm)
```
```{r plot cluster groups, echo=FALSE}
plotcluster(dataWithoutGroups,km$cluster)
```


```{r plot cluster groups 2, echo=FALSE}
fviz_cluster(km, data = dataWithoutGroups,geom = "point", ellipse.type = "norm")
```
```{r kmeans}
km<-kmeans(datos,2,iter.max =100)
#datos$
#datos$grupoKm<-km$cluster
```
```{r k plot}
plotcluster(datos,km$cluster)
```


```{r plot2}
fviz_cluster(km, data = datos,geom = "point", ellipse.type = "norm")
```



```{r silueta k mean, echo=FALSE}
silkm<-silhouette(km$cluster,dist(dataWithoutGroups))
plot(silkm, cex.names=.4, col=1:3)
```

Por medio del metodo de la siluera logramos encontrar que

```{r cluset jerarquico, echo=FALSE}
#datos <- movies[,c("popularity", "budget", "revenue", "runtime", "genresAmount", "voteCount", "voteAvg", "castMenAmount", "castWomenAmount")]

#datos <- scale(datos)
#datos <- na.omit(datos)

matriz_dist<- dist(dataWithoutGroups)
datos_dist<- dist(dataWithoutGroups)
hc<-hclust(datos_dist, method = "ward.D2")
plot(hc, cex=0.5, axes=FALSE) #Genera el dendograma
rect.hclust(hc,k=3)
```
```{r jerarquico, echo=FALSE}
groups<-cutree(hc,k=3)

datos$gruposHC<-groups

#dataWithoutGroups

table(groups)
```

```{r silueta jerarquico, echo=FALSE}
silhc<-silhouette(groups,datos_dist)
mean(silhc[,3]) 
```


```{r grafica, echo=FALSE}
plot(silhc, cex.names=.4, col=1:3)

```

Resumen de las variables con el metodo de k means
```{r summary de los datos, echo=FALSE, warning=FALSE}

movies <- movies[rownames(movies) %in% rownames(datos), ]

movies$grupoKm <- datos$grupoKm  
movies$grupoHC <- datos$gruposHC 


# Resumen estadístico por grupo
movies <- as.data.frame(movies)
movies_clean <- movies %>% dplyr::select(-grupoHC)

summary_by_group <- movies_clean %>%
  group_by(grupoKm) %>%
  summarise(across(where(is.numeric), list(mean = mean, sd = sd), .names = "{.col}_{.fn}"), na.rm = TRUE)


variables_importantes <- c("popularity_mean", "budget_mean", "revenue_mean", "runtime_mean")

kable(summary_by_group %>% dplyr::select(all_of(variables_importantes)))

```

Resumen de las variables con el metodo de cluster jerarquico
```{r summary de los datos jerarquicos, echo=FALSE, warning=FALSE}
summary_by_groupHC <- movies %>%
  group_by(grupoHC) %>%
  summarise(across(where(is.numeric), list(mean = mean, sd = sd), .names = "{.col}_{.fn}"))


variables_importantes <- c("popularity_mean", "budget_mean", "revenue_mean", "runtime_mean")

kable(summary_by_groupHC %>% dplyr::select(all_of(variables_importantes)))
```

## Interpretacion del clustering

K mean por el meotdo de la silueta le quedaba de mejor forma organizarse en 2 grupos y cluster jerarquico por este mismo metodo le quedaba de mejor forma organizarse en 3 grupos.

Teniendo eso en cuenta analizamos procedemos a analizar.

Por lo que se puede observar en el resumen de ambos grupos, los dos metodos tuvieron distribuciones bastante parecidas menos por el presupuesto y los ingresos, los cuales por claras razones de ser 3 y 2 en el cluster jerarquico estan un poco mejor distribuidas.

```{r prespuesto vs revenues, echo=FALSE, warning=FALSE, error=FALSE}
ggplot(movies, aes(x = budget, y = revenue, color = as.factor(grupoKm))) +
  geom_point(alpha = 0.6) +
  labs(title = "Relación entre Presupuesto y Revenue por Cluster", x = "Presupuesto", y = "Revenue") +
  theme_minimal()
```

```{r actor popularidad, echo=FALSE, warning=FALSE, error=FALSE}

movies$actorsPopularityMean <- sapply(movies$actorsPopularity, function(x) {
  values <- as.numeric(unlist(strsplit(x, "\\|"))) 
  mean(values, na.rm = TRUE)
})

ggplot(movies, aes(x = as.factor(grupoKm), y = actorsPopularityMean, fill = as.factor(grupoKm))) +
  geom_boxplot() +
  labs(title = "Popularidad Promedio del Elenco por Cluster", x = "Cluster", y = "Media de Popularidad Actores") +
  theme_minimal()

```
```{r a}
silkm<-silhouette(km$cluster,dist(datos))
plot(silkm, cex.names=.4, col=1:3)
```

--- 

# Analisis por Componentes principales

Para ello primero se requirio de la limpieza del dataset para contar solamente contar con un dataframe de variables cuantitativas. Este sería un ejemplo del Dataset con el que se trabajará el PCA.

Para ello se tomaron solo variables que eran inherentemente cuantitativas, ya que a pesar que las variables cualitativas aunque no se conviertan a un equivalente númerico, no tienen noción de orden y magnitud, y por en no aportando mucho valor al cálculo de componentes.

```{r}
PCA_raw_data_na <- na.omit(movies[,c("popularity", "budget", "revenue", "runtime", "genresAmount", "voteCount", "voteAvg", "castMenAmount", "castWomenAmount")])
PCA_raw_data_na$castMenAmount <- as.numeric(PCA_raw_data_na$castMenAmount)
PCA_raw_data_na$castWomenAmount <- as.numeric(PCA_raw_data_na$castWomenAmount)
PCA_raw_data <- PCA_raw_data_na[complete.cases(PCA_raw_data_na), ]
kable(head(PCA_raw_data))
```

## PCA es aplicable
Acto seguido, es importantes verificar si el PCA será una buena técnica para este conjunto de datos. Empezando por revisar la colinealidad de las variables: 

```{r}
rcor<-cor(PCA_raw_data, use = "pairwise.complete.obs")
corrplot(rcor)
```
Vemos un valor es `r det(rcor)`, que es cercano a 0, y también podemos ver que hay una relación del estrecha (cercana al 60%) entre las ganancias y el presupuesto, así como número de votos y ganancias. Para reafirmar si el PCA será un método correcto, se corrieron tambien los test KMO y Test de esfericidad de Bartlet que arrojan los siguientes indicios:

**KMO**
```{r}
KMO(as.matrix(PCA_raw_data))
```
**Esfericidad de Bartlet**
```{r}
cortest.bartlett(PCA_raw_data)
```
Ambas estadísticas ofrecen resultados favorecedores al indica que si hay relación entre las diferentes variables y datos con con un MSA superior a 0.5 (MSA=0.71) y bartlet indicando que si hay interdependencia entre las variables (p=0). Es decir, podemos procer a aplicar PCA.


## Aplicación de PCA

Estas serian las componentes de principales del subset de datos.
```{r}
sum(is.na(PCA_raw_data))
compPrinc<-prcomp(PCA_raw_data, scale = T)
compPrinc
```
Por la regla de Kaiser (las componentes con desviacion estandar mayor a 1 son las que aportan mayor peso) se desvela que las primeras 77% componentes son las más importantes al explicar el 86% de la variación de los datos:

```{r}
summary(compPrinc)
```
La gŕafica recomienda el uso de 2 componentes, pero para encontrar, eso explicaria solo el 44% de la variabilidad de los datos, así que nos quedaremos en el análisis con componentes: 
```{r screeplot}
fviz_eig(compPrinc, addlabels = TRUE, ylim = c(0, 80))
fviz_eig(compPrinc, addlabels = TRUE, choice = c("eigenvalue"), ylim = c(0, 3))
```
## Interpretación de PSA

El siguiente gráfico muestra vectores proyectos sobre las 2 primeras componentes. Y se puede observar las relaciones descubiertas en pasos anteriorres sobre la covarianza: Las componetnes de presupuesto, y número de votos y ganancias estan muy relacionadas positivamente entre, y bien representadas en la primera dimensión.

También se puede decir algo parecido del tiempo de expiración y el promedio de votos, solo que estos 2 ultimos se encuentran menos representados en la 1 y 2 dimensión.
```{r grafico coseno}
fviz_pca_var(compPrinc, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
)

```
